import os

from chromadb.api.types import IncludeEnum
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM

# ğŸ›  é…ç½®éƒ¨åˆ†
persist_directory = "./chroma_db"  # Chroma æœ¬åœ°æŒä¹…åŒ–ç›®å½•
embeddings = HuggingFaceEmbeddings(
    model_name=r'C:\Users\huangrx\.cache\huggingface\hub\bge-large-zh-v1.5',
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True}
)

# ğŸŸ¢ åˆå§‹åŒ–æœ¬åœ°å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™åŠ è½½ï¼‰
if not os.path.exists(persist_directory):
    os.makedirs(persist_directory)

vector_store = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)

# ğŸŒ LLM åˆå§‹åŒ–
llm = OllamaLLM(
    base_url='http://localhost:11434',
    model='deepseek-r1:14b'
)


# ğŸ“ è®¡ç®—ä¸Šä¼ é¡ºåºçš„å‡½æ•° (è‡ªåŠ¨è®¡ç®— upload_order)
def get_next_upload_order(user_id, session_id):
    # è·å–æ‰€æœ‰æ–‡æ¡£
    results = vector_store._collection.get(
        limit=100,  # é™åˆ¶æ£€ç´¢æ•°é‡
        include=[IncludeEnum.metadatas]
    )

    # è·å–ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£ï¼Œå¹¶è®¡ç®—æœ€å¤§ upload_order
    current_orders = []
    for metadata in results["metadatas"]:
        if metadata.get("user_id") == user_id and metadata.get("session_id") == session_id:
            current_orders.append(metadata.get("upload_order", 0))

    # è®¡ç®—ä¸‹ä¸€ä¸ªä¸Šä¼ é¡ºåº
    next_order = max(current_orders, default=0) + 1
    return next_order


# ğŸ“ ä¸Šä¼ æ–‡æ¡£çš„å‡½æ•° (è‡ªåŠ¨è·å– upload_order)
def upload_file(file_path, user_id, session_id):
    # è‡ªåŠ¨è·å–å½“å‰ç”¨æˆ·å’Œä¼šè¯çš„ä¸‹ä¸€ä¸ªä¸Šä¼ é¡ºåº
    upload_order = get_next_upload_order(user_id, session_id)

    # åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£
    loader = PyPDFLoader(file_path)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)

    # æ·»åŠ å…ƒæ•°æ® (è‡ªåŠ¨è®¡ç®—çš„ upload_order)
    for doc in split_docs:
        doc.metadata["user_id"] = user_id
        doc.metadata["session_id"] = session_id
        doc.metadata["file_name"] = os.path.basename(file_path)
        doc.metadata["upload_order"] = upload_order

    # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–
    vector_store.add_documents(split_docs)
    print(f"ğŸ“¥ æ–‡ä»¶ '{file_path}' å·²ä¸Šä¼ å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ï¼ (upload_order={upload_order})")


# ğŸ” ç”¨æˆ·æŸ¥è¯¢çš„å‡½æ•°
def query_user_data(user_id, session_id=None, file_name=None, upload_order=None, query_text=""):
    # æ„å»ºæ£€ç´¢è¿‡æ»¤æ¡ä»¶
    filter_conditions = [{"user_id": user_id}]

    if session_id:
        filter_conditions.append({"session_id": session_id})
    if file_name:
        filter_conditions.append({"file_name": file_name})
    if upload_order:
        filter_conditions.append({"upload_order": {"$eq": upload_order}})

    # ä½¿ç”¨ $and ç»„åˆæ‰€æœ‰æ¡ä»¶
    filter_condition = {"$and": filter_conditions} if len(filter_conditions) > 1 else filter_conditions[0]

    # ä»…æ£€ç´¢ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£
    retriever = vector_store.as_retriever(search_kwargs={"filter": filter_condition})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    result = qa_chain.invoke({"query": query_text})

    print("\nğŸ’¬ é—®é¢˜:", query_text)
    print("ğŸ“– å›ç­”:", result['result'])

    # å±•ç¤ºç›¸å…³æ–‡æ¡£ä¿¡æ¯
    for doc in result['source_documents']:
        print(f"\nğŸ“ æ¥æºæ–‡ä»¶: {doc.metadata.get('file_name', 'æœªçŸ¥')}")
        print(f"ğŸ†” ç”¨æˆ·ID: {doc.metadata.get('user_id', 'æœªçŸ¥')}")
        print(f"ğŸ“„ ä¸Šä¼ é¡ºåº: {doc.metadata.get('upload_order', 'æœªçŸ¥')}")
        print(f"ğŸ“‘ æ–‡æ¡£ç‰‡æ®µ: {doc.page_content[:200]}...")


# ğŸš€ ç¤ºä¾‹: ä¸Šä¼ å¤šä¸ªç”¨æˆ·çš„æ–‡ä»¶ (è‡ªåŠ¨åˆ†é… upload_order)
# upload_file("äººç‰©ä¼ è®°-æ…•å‹’å°ä¼ .pdf", user_id="user_123", session_id="session_123")
# upload_file("å¤é“ä¹¦æˆ¿-å·´åˆ»æ–‡é›†.pdf", user_id="user_123", session_id="session_123")
# upload_file("äººç”Ÿå­¦æ ¡.pdf", user_id="user_456", session_id="session_125")

# ğŸ” ç¤ºä¾‹: æŸ¥è¯¢ç‰¹å®šç”¨æˆ·çš„æ–‡æ¡£
query_user_data(
    user_id="user_123",
    session_id="session_123",
    query_text="ç¬¬ä¸€æ¬¡ä¸Šä¼ çš„æ–‡ä»¶ä¸­ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    upload_order=1
)

# ğŸ” ç¤ºä¾‹: æŸ¥è¯¢ç‰¹å®šä¸Šä¼ é¡ºåºçš„æ–‡æ¡£
query_user_data(
    user_id="user_123",
    session_id="session_123",
    upload_order=2,
    query_text="ç¬¬äºŒæ¬¡ä¸Šä¼ çš„æ–‡ä»¶é‡Œæœ‰å“ªäº›å†…å®¹ï¼Ÿ"
)
